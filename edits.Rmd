---
title: "new edits"
author: "MEDHASWETA SEN"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Importing the dataset
dataset = read.csv('loan_data.csv')

# Encoding the categorical variables as factors
dataset$purpose = as.numeric(factor(dataset$purpose))
dataset$not.fully.paid <- as.numeric(as.character(factor(dataset$not.fully.paid)))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$not.fully.paid, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)

# Fitting XGBoost to the Training set
#install.packages('xgboost')
library(xgboost)
library(caret)
folds = createFolds(training_set$not.fully.paid, k = 10)
cv = lapply(folds, function(x) {
  training_fold = training_set[-x, ]
  test_fold = training_set[x, ]
  classifier = xgboost(data = as.matrix(training_fold[-14]), label = training_fold$not.fully.paid, nrounds = 10)
  y_pred = predict(classifier, newdata = as.matrix(test_fold[-14]))
  y_pred = (y_pred >= 0.5)
  cm = table(test_fold[, 14], y_pred)
  accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
  return(accuracy)
})
xgb_acc = mean(as.numeric(cv))
```
```{r}
#Logistic Regression

# Feature Scaling
dataset$not.fully.paid <- factor(dataset$not.fully.paid)
training_set[-14] = scale(training_set[-14])
test_set[-14] = scale(test_set[-14])
folds = createFolds(training_set$not.fully.paid, k = 10)
cv = lapply(folds, function(x) {
  training_fold = training_set[-x, ]
  test_fold = training_set[x, ]
  classifier = glm(formula = not.fully.paid ~ credit.policy+purpose+int.rate+installment+log.annual.inc+fico+
                     days.with.cr.line+revol.bal+inq.last.6mths+delinq.2yrs,
                   family = binomial,
                   data = training_fold)
  prob_pred = predict(classifier, type = 'response', newdata = test_fold[-14])
  y_pred = ifelse(prob_pred > 0.5, 1, 0)
  cm = table(test_fold[, 14], y_pred)
  accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
  return(accuracy)
})
logistic_acc = mean(as.numeric(cv))

dataset$credit.policy <- factor(dataset$credit.policy)
training_set[-1] = scale(training_set[-1])
test_set[-1] = scale(test_set[-1])
cv1 = lapply(folds, function(x) {
  training_fold = training_set[-x, ]
  test_fold = training_set[x, ]
  classifier = glm(formula = credit.policy ~ not.fully.paid+purpose+int.rate+installment+log.annual.inc+fico+
                     days.with.cr.line+revol.bal+inq.last.6mths+delinq.2yrs,
                   family = binomial,
                   data = training_fold)
  prob_pred = predict(classifier, type = 'response', newdata = test_fold[-14])
  y_pred = ifelse(prob_pred > 0.5, 1, 0)
  cm = table(test_fold[, 14], y_pred)
  accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
  return(accuracy)
})
logistic_acc = mean(as.numeric(cv1))
```

```{r}
#Kernel SVM
library(e1071)
cv = lapply(folds, function(x) {
  training_fold = training_set[-x, ]
  test_fold = training_set[x, ]
  classifier = svm(formula = not.fully.paid ~ .,
                   data = training_fold,
                   type = 'C-classification',
                   kernel = 'radial')
  y_pred = predict(classifier, newdata = test_fold[-14])
  cm = table(test_fold[, 14], y_pred)
  accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
  return(accuracy)
})
svm_acc = mean(as.numeric(cv))
```

```{r}

#Naive Bayes
cv = lapply(folds, function(x) {
  training_fold = training_set[-x, ]
  test_fold = training_set[x, ]
  classifier = naiveBayes(x = training_fold[-14],
                          y = training_fold$not.fully.paid)
  y_pred = predict(classifier, newdata = test_fold[-14])
  cm = table(test_fold[, 14], y_pred)
  accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
  return(accuracy)
})
Naive_acc = mean(as.numeric(cv))
```
```{r}
#KNN
dataset = read.csv('loan_data.csv')
dataset$purpose = as.numeric(factor(dataset$purpose))
dataset$not.fully.paid <- factor(dataset$not.fully.paid)
set.seed(123)
split = sample.split(dataset$not.fully.paid, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-14] = scale(training_set[-14])
test_set[-14] = scale(test_set[-14])
#PCA
pca = preProcess(x = training_set[-14], method = 'pca', pcaComp = 2)
training_set = predict(pca, training_set)
training_set = training_set[c(2, 3, 1)]
test_set = predict(pca, test_set)
test_set = test_set[c(2, 3, 1)]
```
```{r}
#Tunning
classifier = train(form = not.fully.paid ~ ., data = training_set, method = 'kknn')
classifier$bestTune
library(class)

cv = lapply(folds, function(x) {
  training_fold = training_set[-x, ]
  test_fold = training_set[x, ]
  y_pred = knn(train = training_fold[, -3],
               test = test_fold[, -3],
               cl = training_fold[, 3],
               k = 9,
               prob = TRUE)
  cm = table(test_fold[, 3], y_pred)
  accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
  return(accuracy)
})
knn_acc = mean(as.numeric(cv))

#Plotting The Result
Algorithm <- c("K-NN","Logistic Regression","Naive Bayes",
               "Kernel-SVM","XGB")
Accuracy <- c(knn_acc,logistic_acc,Naive_acc,svm_acc,xgb_acc)
Result <- data.frame(Algorithm,Accuracy)
library('ggplot2')
f<-ggplot(data=Result,aes(y=Accuracy,x=reorder(Algorithm,Accuracy))) + geom_col(width = 0.5)
f + ggtitle("Algorithms Performance \n (Ascending Order)") +
  xlab("Algorithm") + ylab("Accuracy")
```

