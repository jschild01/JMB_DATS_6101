kable() %>%
kable_styling(bootstrap_options = c("striped", "condensed"), full_width = FALSE)
## Can we bold the values in the first column ("Actual") using kable??
#Creating a table for the confusion matrix statistics
# Start by converting to a data.frame
cm_stats <- as.data.frame(confusion_matrix$byClass) %>%
rownames_to_column()
# Adjust the column names and values to look better
colnames(cm_stats) <- c("Statistic", "Percentage")
cm_stats$Percentage <- paste0(round(cm_stats$Percentage*100, digits=1), "%")
# Output a nice table
# xkabledply(cm_stats, title = "Confusion matrix statistics for the tree model")
cm_stats %>%
kable() %>%
kable_styling(bootstrap_options = c("striped", "condensed"), full_width = FALSE)
library(pROC)
# loans_tree$prediction <- predict(creditfit, type = "vector")
loans_tree$prediction <- predict(creditfit, type = "prob")[,2]
tree_roc <- roc(credit.policy ~ prediction, data=loans_tree)
auc(tree_roc) # area-under-curve prefer 0.8 or higher.
plot(tree_roc)
# fit linear model
linear_model <- lm(int.rate ~ fico, data=loans)
# view summary of linear model
summary(linear_model)
# scatterplot with regression line and confidence interval
# scatterplot(int.rate ~ fico, data = loans)
loans %>%
ggplot(aes(x = fico, y = int.rate)) +
geom_point(color = "steelblue", alpha = 0.2) +
geom_smooth(method = "lm", se = T) +
labs(title = "Interest Rate vs FICO Score",
x = "FICO Score", y = "Interest Rate") +
scale_x_continuous(limits = c(600, NA), expand = expansion(mult = c(0, .05))) +
scale_y_continuous(labels = label_percent(), limits = c(.05, NA), expand = expansion(mult = c(0, .05))) +
theme_minimal()
# fit linear model
linear_model <- lm(revol.util ~ int.rate, data=loans)
# view summary of linear model
summary(linear_model)
# scatterplot with regression line and confidence interval
# scatterplot(revol.util ~ int.rate, data = loans)
loans %>%
ggplot(aes(x = int.rate, y = revol.util)) +
geom_point(color = "steelblue", alpha = 0.2) +
geom_smooth(method = "lm") +
labs(title = "Revolving Line Utilization Rate vs Interest Rate",
x = "Interest Rate", y = "Revolving Line Utilization Rate") +
scale_x_continuous(labels = label_percent(), limits = c(.05, NA), expand = expansion(mult = c(0, .05))) +
scale_y_continuous(labels = label_percent(scale = 1)) +
theme_minimal()
# fit linear model
linear_model <- lm(installment ~ log.annual.inc, data=loans)
# view summary of linear model
summary(linear_model)
# scatterplot with regression line and confidence interval
# scatterplot(installment ~ log.annual.inc, data = loans)
loans %>%
ggplot(aes(x = log.annual.inc, y = installment)) +
geom_point(color = "steelblue", alpha = 0.2) +
geom_smooth(method = "lm") +
labs(title = "Installment vs Log of Annual Income",
x = "Log of Annual Income", y = "Installment") +
theme_minimal()
# fit linear model
linear_model <- lm(revol.util ~ fico, data=loans)
# view summary of linear model
summary(linear_model)
# scatterplot with regression line and confidence interval
# scatterplot(revol.util ~ fico, data = loans)
loans %>%
ggplot(aes(x = fico, y = revol.util)) +
geom_point(color = "steelblue", alpha = 0.2) +
geom_smooth(method = "lm") +
labs(title = "Revolving Line Utilization Rate vs FICO Score",
x = "FICO Score", y = "Revolving Line Utilization Rate") +
scale_x_continuous(limits = c(600, NA), expand = expansion(mult = c(0, .05))) +
scale_y_continuous(labels = label_percent(scale = 1)) +
theme_minimal()
credit.policypurposetable = xtabs(~ credit.policy + purpose, data = loans)
credit.policypurposetable
set.seed(7)
# load the library
library(mlbench)
library(caret)
# load the dataset
# prepare training scheme
loans$credit.policy = as.factor(loans$credit.policy)
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
class(loans$credit.policy)
model <- train(credit.policy~., data=loans, method="glm", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)
#names(getModelInfo())
loans$credit.policy <- factor(loans$credit.policy)
str(loans)
loans$purpose <- factor(loans$purpose)
#str(credit.policy)
credit.policyLogit <- glm(credit.policy ~ . , data = loans, binomial(link = "logit") )
summary(credit.policyLogit)
xkabledply(credit.policyLogit, title = "Logistic Regression :")
p_fitted = credit.policyLogit$fitted.values[1] # this is the model predicated value p-hat for the first data row (not the actual data point p)
# This gives you the predicted values of the data points inside the model.
predict(credit.policyLogit)  # the is from the model, which gives you the value for logit(p) or ln(p/q)
## CIs using profiled log-likelihood
# confint(credit.policyLogit)
xkabledply( confint(credit.policyLogit), title = "CIs using profiled log-likelihood" )
## CIs using standard errors
# confint.default(credit.policyLogit)
xkabledply( confint.default(credit.policyLogit), title = "CIs using standard errors" )
loadPkg("regclass")
# confusion_matrix(credit.policyLogit)
xkabledply( confusion_matrix(credit.policyLogit), title = "Confusion matrix from Logit Model" )
unloadPkg("regclass")
loadPkg("pROC") # receiver operating characteristic curve, gives the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The curve is on sensitivity/recall/true-positive-rate vs false_alarm/false-positive-rate/fall-out.
prob=predict(credit.policyLogit, type = "response" )
loans$prob <- NA
loans$prob=prob
h <- roc(credit.policy~prob, data=loans)
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h)
# unloadPkg("pROC")
head(loans)
credit.policyNullLogit <- glm(credit.policy ~ 1, data = loans, family = "binomial")
mcFadden = 1 - logLik(credit.policyLogit)/logLik(credit.policyNullLogit)
mcFadden
loadPkg("pscl") # use pR2( ) function to calculate McFadden statistics for model eval
credit.policyLogitpr2 = pR2(credit.policyLogit)
credit.policyLogitpr2
unloadPkg("pscl")
loadPkg("ResourceSelection") # function hoslem.test( ) for logit model evaluation
credit.policyLogitHoslem = hoslem.test(loans$credit.policy, fitted(credit.policyLogit)) # Hosmer and Lemeshow test, a chi-squared test
credit.policyLogitHoslem
# Have not found a good way to display it.
not.fully.paidpurposetable = xtabs(~ not.fully.paid + purpose, data = loans)
not.fully.paidpurposetable
set.seed(7)
# load the library
library(mlbench)
library(caret)
loans$not.fully.paid = as.factor(loans$not.fully.paid)
# load the dataset
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
class(loans$not.fully.paid)
model <- train(not.fully.paid~., data=loans, method="glm", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)
#names(getModelInfo())
loans$not.fully.paid <- factor(loans$not.fully.paid)
str(loans)
loans$purpose <- factor(loans$purpose)
#str(not.fully.paid)
not.fully.paidLogit <- glm(not.fully.paid ~., data = loans, binomial(link = "logit") )
summary(not.fully.paidLogit)
xkabledply(not.fully.paidLogit, title = "Logistic Regression :")
p_fitted = not.fully.paidLogit$fitted.values[1] # this is the model predicated value p-hat for the first data row (not the actual data point p)
p_fitted
# This gives you the predicted values of the data points inside the model.
predict(not.fully.paidLogit)  # the is from the model, which gives you the value for logit(p) or ln(p/q)
## CIs using profiled log-likelihood
# confint(not.fully.paidLogit)
xkabledply( confint(not.fully.paidLogit), title = "CIs using profiled log-likelihood" )
## CIs using standard errors
# confint.default(not.fully.paidLogit)
xkabledply( confint.default(not.fully.paidLogit), title = "CIs using standard errors" )
loadPkg("regclass")
# confusion_matrix(not.fully.paidLogit)
xkabledply( confusion_matrix(not.fully.paidLogit), title = "Confusion matrix from Logit Model" )
unloadPkg("regclass")
loadPkg("pROC") # receiver operating characteristic curve, gives the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The curve is on sensitivity/recall/true-positive-rate vs false_alarm/false-positive-rate/fall-out.
prob=predict(not.fully.paidLogit, type = "response" )
loans$prob <- NA
loans$prob=prob
h <- roc(not.fully.paid~prob, data=loans)
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h)
# unloadPkg("pROC")
head(loans)
not.fully.paidNullLogit <- glm(not.fully.paid ~ 1, data = loans, family = "binomial")
mcFadden = 1 - logLik(not.fully.paidLogit)/logLik(not.fully.paidNullLogit)
mcFadden
loadPkg("pscl") # use pR2( ) function to calculate McFadden statistics for model eval
not.fully.paidLogitpr2 = pR2(not.fully.paidLogit)
not.fully.paidLogitpr2
unloadPkg("pscl")
loadPkg("ResourceSelection") # function hoslem.test( ) for logit model evaluation
not.fully.paidLogitHoslem = hoslem.test(loans$not.fully.paid, fitted(not.fully.paidLogit)) # Hosmer and Lemeshow test, a chi-squared test
not.fully.paidLogitHoslem
# Have not found a good way to display it.
# We want the results to be hidden by default, though for some chunks we will override this to show the results
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
library(ezids) # We will use functions form this package to get nicer looking results
library(tidyverse) # We need this package for data manipulation, piping, and graphing
library(corrplot) # We will need this package to plot the correlation matrix
library(scales) # This package will help us when labeling the scatter plots
# library(gridExtra) # For additional table and image functionality
# library(expss)
library(knitr)
library(kableExtra)
# library(broom)
# library(purrr)
library(rpart) # For the classification tree model
library(caret) # For the classification tree model
set.seed(1) # For the classification tree model
library(rpart.plot) # For plotting nice looking classification trees
library(pROC) # For ROC/AUC
library(car)
# Read in the data from the working directory
loans <- read_csv("loan_data.csv")
#loans
# We determined this makes sense to do from our EDA
str(loans) # keep??
loans$credit.policy <- as.logical(loans$credit.policy)
loans$not.fully.paid <- as.logical(loans$not.fully.paid)
loans$purpose <- as.factor(loans$purpose)
# It might help to turn these variables into logicals since most are 0
loans <- loans %>%
mutate(has.delinq.2yrs = delinq.2yrs > 0,
has.pub.rec = pub.rec > 0)
# Some loans have a revol.bal value of 0, the natural log of which is -Inf
# Adding 1 to all values before taking the natural log resolves this issue by nudging the value a very small amount
# In case it was meaningful if the loan had a revol.bal value of 0 versus >0, we can add a logical to account for that
loans <- loans %>%
mutate(log.revol.bal = log(revol.bal+1),
has.revol.bal = revol.bal > 0)
# A new correlation matrix with the new and transformed variables
# For our correlation matrix we want to include everything but the purpose variable
# We can put the new variables together at the top
loans_correlation_matrix_new <- loans %>%
select(-purpose) %>%
select(revol.bal, log.revol.bal, has.revol.bal, has.delinq.2yrs, has.pub.rec, everything()) %>%
cor()
# The mixed correlation plot makes a nice visualization
corrplot.mixed(loans_correlation_matrix_new, tl.pos = "lt")
# By gathering the variables we want to see into a long format with the gather() function, we can then create a histogram
# for each variable using the facet_wrap() function in ggplot2.
loans %>%
select(int.rate, installment, log.annual.inc, dti, fico, days.with.cr.line, revol.bal, revol.util,
inq.last.6mths, delinq.2yrs, pub.rec, log.revol.bal) %>%
gather(variable, value) %>%
ggplot(aes(x = value)) +
geom_histogram(fill = "steelblue", color = "black") +
facet_wrap(~ variable, scales = "free") + # Free scales so the graphs are readable
labs(title = "Histograms of Numeric Variables", x = "Value", y = "Count") +
theme_minimal()
# By gathering the variables we want to see into a long format with the gather() function, we can then create a boxplot
# for each variable using the facet_wrap() function in ggplot2.
loans %>%
select(int.rate, installment, log.annual.inc, dti, fico, days.with.cr.line, revol.bal, revol.util,
inq.last.6mths, delinq.2yrs, pub.rec, log.revol.bal) %>%
gather(variable, value) %>%
ggplot(aes(x = value)) +
geom_boxplot(fill = "steelblue", color = "black",
outlier.size = 2, outlier.alpha = 0.2) + # Translucent and larger outliers to help with overplotting
facet_wrap(~ variable, scales = "free") + # Free scales so the graphs are readable
labs(title = "Boxplots of Numeric Variables", x = "Value") +
theme_minimal() +
theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())
# By gathering the variables we want to see into a long format with the gather() function, we can then create a Q-Q plot
# for each variable using the facet_wrap() function in ggplot2.
loans %>%
select(int.rate, installment, log.annual.inc, dti, fico, days.with.cr.line, revol.bal, revol.util,
inq.last.6mths, delinq.2yrs, pub.rec, log.revol.bal) %>%
gather(variable, value) %>%
ggplot(aes(sample = value)) +
geom_qq(color = "steelblue") +
geom_qq_line() +
facet_wrap(~ variable, scales = "free") + # Free scales so the graphs are readable
labs(title = "Q-Q Plots of Numeric Variables", x = "Theoretical", y = "Sample") +
theme_minimal()
# Since we are renaming some values let's make a separate dataset for this model
loans_tree <- loans
# First convert the credit.policy variable from a logical to a factor, then rename the levels
loans_tree$credit.policy <- as.factor(loans_tree$credit.policy) %>%
fct_recode(Meets = "TRUE", Fails = "FALSE")
# Max depth of 4 based on Professor Faruqe's instructions
creditfit <- rpart(credit.policy ~ int.rate + fico + inq.last.6mths, data=loans_tree, method="class", control = list(maxdepth = 4) )
printcp(creditfit) # display the results
plotcp(creditfit) # visualize cross-validation results
# Show these results??
summary(creditfit) # detailed summary of splits
# plot the tree and add text
rpart.plot(creditfit, uniform=TRUE, main="Classification Tree for Credit.Policy", digits = 3, extra = 1)
#plot(creditfit, uniform=TRUE, main="Classification Tree for credit.policy")
#text(creditfit, use.n=TRUE, all=TRUE, cex=.75)
# create a postscript plot of tree
# The rpart.plot() function looks quite nice, so we may not need this
post(creditfit, file = "credittree2.ps", title = "Classification Tree for credit.policy")
# Creating the confusion matrix
confusion_matrix = confusionMatrix(predict(creditfit, type = "class"), reference = loans_tree$credit.policy)
print('Overall: ')
confusion_matrix$overall
print('Class: ')
confusion_matrix$byClass
#The Kappa statistic (or value) is a metric that compares an Observed Accuracy with an Expected Accuracy (random chance). The kappa statistic is used not only to evaluate a single classifier, but also to evaluate classifiers amongst themselves.According to their scheme a value < 0 is indicating no agreement , 0–0.20 as slight, 0.21–0.40 as fair, 0.41–0.60 as moderate, 0.61–0.80 as substantial, and 0.81–1 as almost perfect agreement.
# Creating the confusion matrix table
# To get a table that follows the same format as our notes, with the rows representing true values and columns representing predicted values, we will have to adjust the data a bit.
# Start by converting to a data.frame
cm_table <- as.data.frame(confusion_matrix$table) %>%
rename(Actual = "Reference")
# Make it so that the rows represent true values and columns represent predicted values
cm_table$Prediction <- paste0("Prediction - ", cm_table$Prediction)
cm_table <- spread(cm_table, Prediction, Freq)
# Output a nice table
# xkabledply(cm_table, title = "Confusion matrix for the tree model")
cm_table %>%
kable() %>%
kable_styling(bootstrap_options = c("striped", "condensed"), full_width = FALSE)
## Can we bold the values in the first column ("Actual") using kable??
#Creating a table for the confusion matrix statistics
# Start by converting to a data.frame
cm_stats <- as.data.frame(confusion_matrix$byClass) %>%
rownames_to_column()
# Adjust the column names and values to look better
colnames(cm_stats) <- c("Statistic", "Percentage")
cm_stats$Percentage <- paste0(round(cm_stats$Percentage*100, digits=1), "%")
# Output a nice table
# xkabledply(cm_stats, title = "Confusion matrix statistics for the tree model")
cm_stats %>%
kable() %>%
kable_styling(bootstrap_options = c("striped", "condensed"), full_width = FALSE)
library(pROC)
# loans_tree$prediction <- predict(creditfit, type = "vector")
loans_tree$prediction <- predict(creditfit, type = "prob")[,2]
tree_roc <- roc(credit.policy ~ prediction, data=loans_tree)
auc(tree_roc) # area-under-curve prefer 0.8 or higher.
plot(tree_roc)
# fit linear model
linear_model <- lm(int.rate ~ fico, data=loans)
# view summary of linear model
summary(linear_model)
# scatterplot with regression line and confidence interval
# scatterplot(int.rate ~ fico, data = loans)
loans %>%
ggplot(aes(x = fico, y = int.rate)) +
geom_point(color = "steelblue", alpha = 0.2) +
geom_smooth(method = "lm", se = T) +
labs(title = "Interest Rate vs FICO Score",
x = "FICO Score", y = "Interest Rate") +
scale_x_continuous(limits = c(600, NA), expand = expansion(mult = c(0, .05))) +
scale_y_continuous(labels = label_percent(), limits = c(.05, NA), expand = expansion(mult = c(0, .05))) +
theme_minimal()
# fit linear model
linear_model <- lm(revol.util ~ int.rate, data=loans)
# view summary of linear model
summary(linear_model)
# scatterplot with regression line and confidence interval
# scatterplot(revol.util ~ int.rate, data = loans)
loans %>%
ggplot(aes(x = int.rate, y = revol.util)) +
geom_point(color = "steelblue", alpha = 0.2) +
geom_smooth(method = "lm") +
labs(title = "Revolving Line Utilization Rate vs Interest Rate",
x = "Interest Rate", y = "Revolving Line Utilization Rate") +
scale_x_continuous(labels = label_percent(), limits = c(.05, NA), expand = expansion(mult = c(0, .05))) +
scale_y_continuous(labels = label_percent(scale = 1)) +
theme_minimal()
# fit linear model
linear_model <- lm(installment ~ log.annual.inc, data=loans)
# view summary of linear model
summary(linear_model)
# scatterplot with regression line and confidence interval
# scatterplot(installment ~ log.annual.inc, data = loans)
loans %>%
ggplot(aes(x = log.annual.inc, y = installment)) +
geom_point(color = "steelblue", alpha = 0.2) +
geom_smooth(method = "lm") +
labs(title = "Installment vs Log of Annual Income",
x = "Log of Annual Income", y = "Installment") +
theme_minimal()
# fit linear model
linear_model <- lm(revol.util ~ fico, data=loans)
# view summary of linear model
summary(linear_model)
# scatterplot with regression line and confidence interval
# scatterplot(revol.util ~ fico, data = loans)
loans %>%
ggplot(aes(x = fico, y = revol.util)) +
geom_point(color = "steelblue", alpha = 0.2) +
geom_smooth(method = "lm") +
labs(title = "Revolving Line Utilization Rate vs FICO Score",
x = "FICO Score", y = "Revolving Line Utilization Rate") +
scale_x_continuous(limits = c(600, NA), expand = expansion(mult = c(0, .05))) +
scale_y_continuous(labels = label_percent(scale = 1)) +
theme_minimal()
set.seed(7)
# load the library
library(mlbench)
library(caret)
# load the dataset
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
class(df$not.fully.paid)
model <- train(int.rate~., data=df, method="lm", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)
#names(getModelInfo())
model <- lm(int.rate ~ fico+installment+purpose+revol.util+log.revol.bal+credit.policy+log.annual.inc, data = df)
summary(model)
model$coefficients
model$call
coef(model)
confint(model)
library("broom")
tidyfinal <-  tidy(model)
tidyfinal
Model_Summary <- augment(model)
str(Model_Summary)
head(Model_Summary)
par(mfrow=c(2,2))
plot(fit1)
knitr::opts_chunk$set(echo = TRUE)
loans <- read.csv("without na.csv")
head(loans)
summary(loans)
str(loans)
library(car)
set.seed(7)
# load the library
library(mlbench)
library(caret)
# load the dataset
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
class(loans$not.fully.paid)
model <- train(int.rate~., data=loans, method="lm", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)
#names(getModelInfo())
model <- lm(int.rate ~ fico+installment+purpose+revol.util+log.revol.bal+credit.policy+log.annual.inc, data = loans)
summary(model)
model$coefficients
model$call
coef(model)
confint(model)
library("broom")
tidyfinal <-  tidy(model)
tidyfinal
Model_Summary <- augment(model)
str(Model_Summary)
head(Model_Summary)
par(mfrow=c(2,2))
plot(fit1)
knitr::opts_chunk$set(echo = TRUE)
loans <- read.csv("without na.csv")
head(loans)
summary(loans)
str(loans)
library(car)
set.seed(7)
# load the library
library(mlbench)
library(caret)
# load the dataset
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
class(loans$not.fully.paid)
model <- train(int.rate~., data=loans, method="lm", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)
#names(getModelInfo())
model <- lm(int.rate ~ fico+installment+purpose+revol.util+log.revol.bal+credit.policy+log.annual.inc, data = loans)
summary(model)
model$coefficients
model$call
coef(model)
confint(model)
library("broom")
tidyfinal <-  tidy(model)
tidyfinal
Model_Summary <- augment(model)
str(Model_Summary)
head(Model_Summary)
par(mfrow=c(2,2))
plot(model)
vif(model)
lmtest::bptest(model)
car::ncvTest(model)
distBCMod <- caret::BoxCoxTrans(loans$int.rate)
print(distBCMod)
loans <- cbind(loans, dist_new=predict(distBCMod, loans$int.rate))
head(loans)
mod1 <- lm(dist_new ~ fico+installment+purpose+revol.util+log.revol.bal+credit.policy+log.annual.inc, data=loans)
summary(mod1)
lmtest::bptest(mod1)
plot(mod1)
