# unloadPkg("pROC")
head(df)
not.fully.paidNullLogit <- glm(not.fully.paid ~ 1, data = df, family = "binomial")
mcFadden = 1 - logLik(not.fully.paidLogit)/logLik(not.fully.paidNullLogit)
mcFadden
loadPkg("pscl") # use pR2( ) function to calculate McFadden statistics for model eval
not.fully.paidLogitpr2 = pR2(not.fully.paidLogit)
not.fully.paidLogitpr2
unloadPkg("pscl")
loadPkg("ResourceSelection") # function hoslem.test( ) for logit model evaluation
not.fully.paidLogitHoslem = hoslem.test(df$not.fully.paid, fitted(not.fully.paidLogit)) # Hosmer and Lemeshow test, a chi-squared test
not.fully.paidLogitHoslem
# Have not found a good way to display it.
# The package "ezids" (EZ Intro to Data Science) includes a lot of the helper functions we developed for the course.
# Some of the frequently used functions are loadPkg(), xkabledply(), xkablesummary(), uzscale(), etc.
library(ezids)
# some of common options (and the defaults) are:
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right',
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3)
# options(scipen=9, digits = 3)
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
loadPkg("ggplot2")
# not.fully.paid <- data.frame(read.csv("LogRegnot.fully.paid.csv"))
df <- read.csv("without na.csv")
head(df)
summary(df)
str(df)
sum(is.na(df))
df$not.fully.paid=as.numeric(df$not.fully.paid)
xkablesummary(df)
# find  sd for all columns, using sapply
varSD = sapply(df, sd)
varSD
xkabledply(as.table(varSD), title = "The sd for each variable in not.fully.paid", wide = TRUE)
not.fully.paidpurposetable = xtabs(~ not.fully.paid + purpose, data = df)
not.fully.paidpurposetable
set.seed(7)
# load the library
library(mlbench)
library(caret)
# load the dataset
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
class(df$not.fully.paid)
model <- train(not.fully.paid~., data=df, method="glm", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)
#names(getModelInfo())
df$not.fully.paid <- factor(df$not.fully.paid)
str(df)
df$purpose <- factor(df$purpose)
#str(not.fully.paid)
not.fully.paidLogit <- glm(not.fully.paid ~inq.last.6mths+installment+log.annual.inc+credit.policy+purpose+fico+pub.rec+log.revol.bal, data = df, binomial(link = "logit") )
summary(not.fully.paidLogit)
xkabledply(not.fully.paidLogit, title = "Logistic Regression :")
p_fitted = not.fully.paidLogit$fitted.values[1] # this is the model predicated value p-hat for the first data row (not the actual data point p)
# This gives you the predicted values of the data points inside the model.
predict(not.fully.paidLogit)  # the is from the model, which gives you the value for logit(p) or ln(p/q)
## CIs using profiled log-likelihood
# confint(not.fully.paidLogit)
xkabledply( confint(not.fully.paidLogit), title = "CIs using profiled log-likelihood" )
## CIs using standard errors
# confint.default(not.fully.paidLogit)
xkabledply( confint.default(not.fully.paidLogit), title = "CIs using standard errors" )
loadPkg("regclass")
# confusion_matrix(not.fully.paidLogit)
xkabledply( confusion_matrix(not.fully.paidLogit), title = "Confusion matrix from Logit Model" )
unloadPkg("regclass")
loadPkg("pROC") # receiver operating characteristic curve, gives the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The curve is on sensitivity/recall/true-positive-rate vs false_alarm/false-positive-rate/fall-out.
prob=predict(not.fully.paidLogit, type = "response" )
df$prob <- NA
df$prob=prob
h <- roc(not.fully.paid~prob, data=df)
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h)
# unloadPkg("pROC")
head(df)
not.fully.paidNullLogit <- glm(not.fully.paid ~ 1, data = df, family = "binomial")
mcFadden = 1 - logLik(not.fully.paidLogit)/logLik(not.fully.paidNullLogit)
mcFadden
loadPkg("pscl") # use pR2( ) function to calculate McFadden statistics for model eval
not.fully.paidLogitpr2 = pR2(not.fully.paidLogit)
not.fully.paidLogitpr2
unloadPkg("pscl")
loadPkg("ResourceSelection") # function hoslem.test( ) for logit model evaluation
not.fully.paidLogitHoslem = hoslem.test(df$not.fully.paid, fitted(not.fully.paidLogit)) # Hosmer and Lemeshow test, a chi-squared test
not.fully.paidLogitHoslem
# Have not found a good way to display it.
p_fitted
# The package "ezids" (EZ Intro to Data Science) includes a lot of the helper functions we developed for the course.
# Some of the frequently used functions are loadPkg(), xkabledply(), xkablesummary(), uzscale(), etc.
library(ezids)
# some of common options (and the defaults) are:
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right',
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3)
# options(scipen=9, digits = 3)
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
loadPkg("ggplot2")
# not.fully.paid <- data.frame(read.csv("LogRegnot.fully.paid.csv"))
df <- read.csv("without na.csv")
head(df)
summary(df)
str(df)
sum(is.na(df))
df$not.fully.paid=as.numeric(df$not.fully.paid)
xkablesummary(df)
# find  sd for all columns, using sapply
varSD = sapply(df, sd)
varSD
xkabledply(as.table(varSD), title = "The sd for each variable in not.fully.paid", wide = TRUE)
not.fully.paidpurposetable = xtabs(~ not.fully.paid + purpose, data = df)
not.fully.paidpurposetable
set.seed(7)
# load the library
library(mlbench)
library(caret)
# load the dataset
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
class(df$not.fully.paid)
model <- train(not.fully.paid~., data=df, method="glm", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)
#names(getModelInfo())
df$not.fully.paid <- factor(df$not.fully.paid)
str(df)
df$purpose <- factor(df$purpose)
#str(not.fully.paid)
not.fully.paidLogit <- glm(not.fully.paid ~.l, data = df, binomial(link = "logit") )
# The package "ezids" (EZ Intro to Data Science) includes a lot of the helper functions we developed for the course.
# Some of the frequently used functions are loadPkg(), xkabledply(), xkablesummary(), uzscale(), etc.
library(ezids)
# some of common options (and the defaults) are:
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right',
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3)
# options(scipen=9, digits = 3)
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
loadPkg("ggplot2")
# not.fully.paid <- data.frame(read.csv("LogRegnot.fully.paid.csv"))
df <- read.csv("without na.csv")
head(df)
summary(df)
str(df)
sum(is.na(df))
df$not.fully.paid=as.numeric(df$not.fully.paid)
xkablesummary(df)
# find  sd for all columns, using sapply
varSD = sapply(df, sd)
varSD
xkabledply(as.table(varSD), title = "The sd for each variable in not.fully.paid", wide = TRUE)
not.fully.paidpurposetable = xtabs(~ not.fully.paid + purpose, data = df)
not.fully.paidpurposetable
set.seed(7)
# load the library
library(mlbench)
library(caret)
# load the dataset
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
class(df$not.fully.paid)
model <- train(not.fully.paid~., data=df, method="glm", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)
#names(getModelInfo())
df$not.fully.paid <- factor(df$not.fully.paid)
str(df)
df$purpose <- factor(df$purpose)
#str(not.fully.paid)
not.fully.paidLogit <- glm(not.fully.paid ~., data = df, binomial(link = "logit") )
summary(not.fully.paidLogit)
xkabledply(not.fully.paidLogit, title = "Logistic Regression :")
p_fitted = not.fully.paidLogit$fitted.values[1] # this is the model predicated value p-hat for the first data row (not the actual data point p)
p_fitted
# This gives you the predicted values of the data points inside the model.
predict(not.fully.paidLogit)  # the is from the model, which gives you the value for logit(p) or ln(p/q)
## CIs using profiled log-likelihood
# confint(not.fully.paidLogit)
xkabledply( confint(not.fully.paidLogit), title = "CIs using profiled log-likelihood" )
## CIs using standard errors
# confint.default(not.fully.paidLogit)
xkabledply( confint.default(not.fully.paidLogit), title = "CIs using standard errors" )
loadPkg("regclass")
# confusion_matrix(not.fully.paidLogit)
xkabledply( confusion_matrix(not.fully.paidLogit), title = "Confusion matrix from Logit Model" )
unloadPkg("regclass")
loadPkg("pROC") # receiver operating characteristic curve, gives the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The curve is on sensitivity/recall/true-positive-rate vs false_alarm/false-positive-rate/fall-out.
prob=predict(not.fully.paidLogit, type = "response" )
df$prob <- NA
df$prob=prob
h <- roc(not.fully.paid~prob, data=df)
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h)
# unloadPkg("pROC")
head(df)
not.fully.paidNullLogit <- glm(not.fully.paid ~ 1, data = df, family = "binomial")
mcFadden = 1 - logLik(not.fully.paidLogit)/logLik(not.fully.paidNullLogit)
mcFadden
loadPkg("pscl") # use pR2( ) function to calculate McFadden statistics for model eval
not.fully.paidLogitpr2 = pR2(not.fully.paidLogit)
not.fully.paidLogitpr2
unloadPkg("pscl")
loadPkg("ResourceSelection") # function hoslem.test( ) for logit model evaluation
not.fully.paidLogitHoslem = hoslem.test(df$not.fully.paid, fitted(not.fully.paidLogit)) # Hosmer and Lemeshow test, a chi-squared test
not.fully.paidLogitHoslem
# Have not found a good way to display it.
# The package "ezids" (EZ Intro to Data Science) includes a lot of the helper functions we developed for the course.
# Some of the frequently used functions are loadPkg(), xkabledply(), xkablesummary(), uzscale(), etc.
library(ezids)
# some of common options (and the defaults) are:
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right',
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3)
# options(scipen=9, digits = 3)
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
loadPkg("ggplot2")
# credit.policy <- data.frame(read.csv("LogRegcredit.policy.csv"))
df <- read.csv("loan_data.csv")
head(df)
summary(df)
str(df)
sum(is.na(df))
df$credit.policy=as.numeric(df$credit.policy)
xkablesummary(df)
# find  sd for all columns, using sapply
varSD = sapply(df, sd)
varSD
xkabledply(as.table(varSD), title = "The sd for each variable in credit.policy", wide = TRUE)
credit.policypurposetable = xtabs(~ credit.policy + purpose, data = df)
credit.policypurposetable
set.seed(7)
# load the library
library(mlbench)
library(caret)
# load the dataset
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
class(df$credit.policy)
model <- train(credit.policy~., data=df, method="glm", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)
#names(getModelInfo())
df$credit.policy <- factor(df$credit.policy)
str(df)
df$purpose <- factor(df$purpose)
#str(credit.policy)
credit.policyLogit <- glm(credit.policy ~ . , data = df, binomial(link = "logit") )
summary(credit.policyLogit)
xkabledply(credit.policyLogit, title = "Logistic Regression :")
p_fitted = credit.policyLogit$fitted.values[1] # this is the model predicated value p-hat for the first data row (not the actual data point p)
# This gives you the predicted values of the data points inside the model.
predict(credit.policyLogit)  # the is from the model, which gives you the value for logit(p) or ln(p/q)
## CIs using profiled log-likelihood
# confint(credit.policyLogit)
xkabledply( confint(credit.policyLogit), title = "CIs using profiled log-likelihood" )
## CIs using standard errors
# confint.default(credit.policyLogit)
xkabledply( confint.default(credit.policyLogit), title = "CIs using standard errors" )
loadPkg("regclass")
# confusion_matrix(credit.policyLogit)
xkabledply( confusion_matrix(credit.policyLogit), title = "Confusion matrix from Logit Model" )
unloadPkg("regclass")
loadPkg("pROC") # receiver operating characteristic curve, gives the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The curve is on sensitivity/recall/true-positive-rate vs false_alarm/false-positive-rate/fall-out.
prob=predict(credit.policyLogit, type = "response" )
df$prob <- NA
df$prob=prob
h <- roc(credit.policy~prob, data=df)
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h)
# unloadPkg("pROC")
head(df)
credit.policyNullLogit <- glm(credit.policy ~ 1, data = df, family = "binomial")
mcFadden = 1 - logLik(credit.policyLogit)/logLik(credit.policyNullLogit)
mcFadden
loadPkg("pscl") # use pR2( ) function to calculate McFadden statistics for model eval
credit.policyLogitpr2 = pR2(credit.policyLogit)
credit.policyLogitpr2
unloadPkg("pscl")
loadPkg("ResourceSelection") # function hoslem.test( ) for logit model evaluation
credit.policyLogitHoslem = hoslem.test(df$credit.policy, fitted(credit.policyLogit)) # Hosmer and Lemeshow test, a chi-squared test
credit.policyLogitHoslem
# Have not found a good way to display it.
# We want the results to be hidden by default, though for some chunks we will override this to show the results
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
library(ezids) # We will use functions form this package to get nicer looking results
library(tidyverse) # We need this package for data manipulation, piping, and graphing
library(corrplot) # We will need this package to plot the correlation matrix
# library(scales) # This package will help us when labeling the scatter plots
# library(gridExtra) # For additional table and image functionality
# library(expss)
library(knitr)
library(kableExtra)
# library(broom)
# library(purrr)
library(rpart) # For the classification tree model
library(caret) # For the classification tree model
set.seed(1) # For the classification tree model
library(rpart.plot) # For plotting nice looking classification trees
library(pROC) # For ROC/AUC
# Read in the data from the working directory
loans <- read_csv("loan_data.csv")
#loans
# We determined this makes sense to do from our EDA
str(loans) # keep??
loans$credit.policy <- as.logical(loans$credit.policy)
loans$not.fully.paid <- as.logical(loans$not.fully.paid)
loans$purpose <- as.factor(loans$purpose)
# It might help to turn these variables into logicals since most are 0
loans <- loans %>%
mutate(has.delinq.2yrs = delinq.2yrs > 0,
has.pub.rec = pub.rec > 0)
# Some loans have a revol.bal value of 0, the natural log of which is -Inf
# Adding 1 to all values before taking the natural log resolves this issue by nudging the value a very small amount
# In case it was meaningful if the loan had a revol.bal value of 0 versus >0, we can add a logical to account for that
loans <- loans %>%
mutate(log.revol.bal = log(revol.bal+1),
has.revol.bal = revol.bal > 0)
# A new correlation matrix with the new and transformed variables
# For our correlation matrix we want to include everything but the purpose variable
# We can put the new variables together at the top
loans_correlation_matrix_new <- loans %>%
select(-purpose) %>%
select(revol.bal, log.revol.bal, has.revol.bal, has.delinq.2yrs, has.pub.rec, everything()) %>%
cor()
# The mixed correlation plot makes a nice visualization
corrplot.mixed(loans_correlation_matrix_new, tl.pos = "lt")
# Using the outlierKD2 function from the ezids package on all numeric variables
loans_with_outliers <- loans
loans <- outlierKD2(loans, days.with.cr.line, rm = TRUE, qqplt = T)
loans <- outlierKD2(loans, delinq.2yrs, rm = TRUE, qqplt = T)
loans <- outlierKD2(loans, dti, rm = TRUE, qqplt = T)
loans <- outlierKD2(loans, fico, rm = TRUE, qqplt = T)
loans <- outlierKD2(loans, inq.last.6mths, rm = TRUE, qqplt = T)
loans <- outlierKD2(loans, installment, rm = TRUE, qqplt = T)
loans <- outlierKD2(loans, int.rate, rm = TRUE, qqplt = T)
loans <- outlierKD2(loans, log.annual.inc, rm = TRUE, qqplt = T)
loans <- outlierKD2(loans, pub.rec, rm = TRUE, qqplt = T)
loans <- outlierKD2(loans, revol.bal, rm = TRUE, qqplt = T)
loans <- outlierKD2(loans, revol.util, rm = TRUE, qqplt = T)
loans <- outlierKD2(loans, log.revol.bal, rm = TRUE, qqplt = T)
# Number of outliers for each variable
loans %>%
is.na() %>%
as.data.frame() %>%
summarize_all(sum) %>%
gather(variable, num_outliers_removed) %>%
kable() %>%
kable_styling(bootstrap_options = c("striped", "condensed"), full_width = FALSE)
# New correlation matrix with the outliers removed
# For our correlation matrix we want to include everything but the purpose variable
loans_correlation_matrix_new_minus_outliers <- loans %>%
select(-purpose) %>%
cor(use = "pairwise.complete.obs")
# The mixed correlation plot makes a nice visualization
corrplot.mixed(loans_correlation_matrix_new_minus_outliers, tl.pos = "lt")
# By gathering the variables we want to see into a long format with the gather() function, we can then create a histogram
# for each variable using the facet_wrap() function in ggplot2.
loans_with_outliers %>%
select(int.rate, installment, log.annual.inc, dti, fico, days.with.cr.line, revol.bal, revol.util,
inq.last.6mths, delinq.2yrs, pub.rec, log.revol.bal) %>%
gather(variable, value) %>%
ggplot(aes(x = value)) +
geom_histogram(fill = "steelblue", color = "black") +
facet_wrap(~ variable, scales = "free") + # Free scales so the graphs are readable
labs(title = "Histograms of Numeric Variables", x = "Value", y = "Count") +
theme_minimal()
# Outliers removed
loans %>%
select(int.rate, installment, log.annual.inc, dti, fico, days.with.cr.line, revol.bal, revol.util,
inq.last.6mths, delinq.2yrs, pub.rec, log.revol.bal) %>%
gather(variable, value) %>%
ggplot(aes(x = value)) +
geom_histogram(fill = "steelblue", color = "black") +
facet_wrap(~ variable, scales = "free") + # Free scales so the graphs are readable
labs(title = "Histograms of Numeric Variables", x = "Value", y = "Count") +
theme_minimal()
# By gathering the variables we want to see into a long format with the gather() function, we can then create a boxplot
# for each variable using the facet_wrap() function in ggplot2.
loans_with_outliers %>%
select(int.rate, installment, log.annual.inc, dti, fico, days.with.cr.line, revol.bal, revol.util,
inq.last.6mths, delinq.2yrs, pub.rec, log.revol.bal) %>%
gather(variable, value) %>%
ggplot(aes(x = value)) +
geom_boxplot(fill = "steelblue", color = "black",
outlier.size = 2, outlier.alpha = 0.2) + # Translucent and larger outliers to help with overplotting
facet_wrap(~ variable, scales = "free") + # Free scales so the graphs are readable
labs(title = "Boxplots of Numeric Variables", x = "Value") +
theme_minimal() +
theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())
# Outliers removed
loans %>%
select(int.rate, installment, log.annual.inc, dti, fico, days.with.cr.line, revol.bal, revol.util,
inq.last.6mths, delinq.2yrs, pub.rec, log.revol.bal) %>%
gather(variable, value) %>%
ggplot(aes(x = value)) +
geom_boxplot(fill = "steelblue", color = "black",
outlier.size = 2, outlier.alpha = 0.2) + # Translucent and larger outliers to help with overplotting
facet_wrap(~ variable, scales = "free") + # Free scales so the graphs are readable
labs(title = "Boxplots of Numeric Variables", x = "Value") +
theme_minimal() +
theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())
# By gathering the variables we want to see into a long format with the gather() function, we can then create a Q-Q plot
# for each variable using the facet_wrap() function in ggplot2.
loans_with_outliers %>%
select(int.rate, installment, log.annual.inc, dti, fico, days.with.cr.line, revol.bal, revol.util,
inq.last.6mths, delinq.2yrs, pub.rec, log.revol.bal) %>%
gather(variable, value) %>%
ggplot(aes(sample = value)) +
geom_qq(color = "steelblue") +
geom_qq_line() +
facet_wrap(~ variable, scales = "free") + # Free scales so the graphs are readable
labs(title = "Q-Q Plots of Numeric Variables", x = "Theoretical", y = "Sample") +
theme_minimal()
# Outliers removed
loans %>%
select(int.rate, installment, log.annual.inc, dti, fico, days.with.cr.line, revol.bal, revol.util,
inq.last.6mths, delinq.2yrs, pub.rec, log.revol.bal) %>%
gather(variable, value) %>%
ggplot(aes(sample = value)) +
geom_qq(color = "steelblue") +
geom_qq_line() +
facet_wrap(~ variable, scales = "free") + # Free scales so the graphs are readable
labs(title = "Q-Q Plots of Numeric Variables", x = "Theoretical", y = "Sample") +
theme_minimal()
# Since we are renaming some values let's make a separate dataset for this model
# loans_tree <- loans
loans_tree <- loans_with_outliers
# First convert the credit.policy variable from a logical to a factor, then rename the levels
loans_tree$credit.policy <- as.factor(loans_tree$credit.policy) %>%
fct_recode(Meets = "TRUE", Fails = "FALSE")
# Max depth of 4 based on Professor Faruqe's instructions
creditfit <- rpart(credit.policy ~ int.rate + fico + inq.last.6mths, data=loans_tree, method="class", control = list(maxdepth = 4) )
printcp(creditfit) # display the results
plotcp(creditfit) # visualize cross-validation results
# Show these results??
summary(creditfit) # detailed summary of splits
# plot the tree and add text
rpart.plot(creditfit, uniform=TRUE, main="Classification Tree for Credit.Policy", digits = 3, extra = 1)
#plot(creditfit, uniform=TRUE, main="Classification Tree for credit.policy")
#text(creditfit, use.n=TRUE, all=TRUE, cex=.75)
# create a postscript plot of tree
# The rpart.plot() function looks quite nice, so we may not need this
post(creditfit, file = "credittree2.ps", title = "Classification Tree for credit.policy")
# Creating the confusion matrix
confusion_matrix = confusionMatrix(predict(creditfit, type = "class"), reference = loans_tree$credit.policy)
print('Overall: ')
confusion_matrix$overall
print('Class: ')
confusion_matrix$byClass
#The Kappa statistic (or value) is a metric that compares an Observed Accuracy with an Expected Accuracy (random chance). The kappa statistic is used not only to evaluate a single classifier, but also to evaluate classifiers amongst themselves.According to their scheme a value < 0 is indicating no agreement , 0–0.20 as slight, 0.21–0.40 as fair, 0.41–0.60 as moderate, 0.61–0.80 as substantial, and 0.81–1 as almost perfect agreement.
# Creating the confusion matrix table
# To get a table that follows the same format as our notes, with the rows representing true values and columns representing predicted values, we will have to adjust the data a bit.
# Start by converting to a data.frame
cm_table <- as.data.frame(confusion_matrix$table) %>%
rename(Actual = "Reference")
# Make it so that the rows represent true values and columns represent predicted values
cm_table$Prediction <- paste0("Prediction - ", cm_table$Prediction)
cm_table <- spread(cm_table, Prediction, Freq)
# Output a nice table
# xkabledply(cm_table, title = "Confusion matrix for the tree model")
cm_table %>%
kable() %>%
kable_styling(bootstrap_options = c("striped", "condensed"), full_width = FALSE)
## Can we bold the values in the first column ("Actual") using kable??
#Creating a table for the confusion matrix statistics
# Start by converting to a data.frame
cm_stats <- as.data.frame(confusion_matrix$byClass) %>%
rownames_to_column()
# Adjust the column names and values to look better
colnames(cm_stats) <- c("Statistic", "Percentage")
cm_stats$Percentage <- paste0(round(cm_stats$Percentage*100, digits=1), "%")
# Output a nice table
# xkabledply(cm_stats, title = "Confusion matrix statistics for the tree model")
cm_stats %>%
kable() %>%
kable_styling(bootstrap_options = c("striped", "condensed"), full_width = FALSE)
library(pROC)
loans_tree$prediction <- predict(creditfit, type = "vector")
# loans_tree$prediction <- predict(creditfit, type = "prob")[,2]
tree_roc <- roc(credit.policy ~ prediction, data=loans_tree)
auc(tree_roc) # area-under-curve prefer 0.8 or higher.
plot(tree_roc)
